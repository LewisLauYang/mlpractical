\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{icml2017}
\citation{JiaYangqing2014LSIR}
\citation{deeplearning2016}
\citation{deeplearning2016}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{clayertable1}{{\caption@xref {clayertable1}{ on input line 65}}{1}{}{table.caption.1}{}}
\citation{cs231n}
\newlabel{Fig.main2}{{1}{2}{convoluation layer implementation\relax }{figure.caption.2}{}}
\newlabel{Fig.main2}{{2}{2}{maxpooling forward propagation implementation\relax }{figure.caption.3}{}}
\newlabel{Fig.main2}{{3}{2}{maxpooling backpropagation implementation\relax }{figure.caption.4}{}}
\citation{deeplearning2016}
\citation{cs231n}
\citation{DBLP:journals/corr/YuK15}
\citation{DBLP:journals/corr/YuK15}
\newlabel{Fig.main2}{{4}{3}{Maxpooling\relax }{figure.caption.5}{}}
\newlabel{Fig.main2}{{5}{3}{receptive field in different layer\relax }{figure.caption.6}{}}
\citation{LecunY.1998Glat}
\citation{LecunY.1998Glat}
\newlabel{Fig.main2}{{6}{4}{LeNet\relax }{figure.caption.7}{}}
\newlabel{tab:addlabel}{{1}{4}{Constant hyperparameter\relax }{table.caption.8}{}}
\newlabel{tab:addlabel}{{2}{4}{Changed hyperparameter\relax }{table.caption.9}{}}
\newlabel{Fig.main2}{{7}{4}{Accuracy of different layers with filter = 64\relax }{figure.caption.10}{}}
\newlabel{Fig.main2}{{8}{4}{Loss of different layers with filter = 64\relax }{figure.caption.11}{}}
\newlabel{Fig.main2}{{9}{5}{Accuracy of different layers with filter = 32\relax }{figure.caption.12}{}}
\newlabel{Fig.main2}{{10}{5}{Loss of different layers with filter = 32\relax }{figure.caption.13}{}}
\newlabel{tab:addlabel}{{3}{5}{Accuracy of Different layers for Test set\relax }{table.caption.14}{}}
\newlabel{tab:addlabel}{{4}{5}{Calculation time per epoch (seconds)\relax }{table.caption.15}{}}
\newlabel{Fig.main2}{{11}{5}{Accuracy of different layers with filter = 32\relax }{figure.caption.16}{}}
\newlabel{Fig.main2}{{12}{5}{Loss of different layers with filter = 32\relax }{figure.caption.17}{}}
\citation{deeplearning2016}
\newlabel{tab:addlabel}{{5}{6}{Accuracy of Different layers for Test set\relax }{table.caption.18}{}}
\newlabel{tab:addlabel}{{6}{6}{one epoch runtime of different layer with filter = 32\relax }{table.caption.19}{}}
\newlabel{sec:concl}{{6}{6}{}{section.6}{}}
\bibdata{example-refs}
\bibcite{cs231n}{{1}{}{{CS231n}}{{}}}
\bibcite{deeplearning2016}{{2}{2016}{{Goodfellow}}{{}}}
\bibcite{JiaYangqing2014LSIR}{{3}{2014}{{Jia}}{{}}}
\bibcite{langley00}{{4}{2000}{{Langley}}{{}}}
\bibcite{LecunY.1998Glat}{{5}{1998}{{Lecun et~al.}}{{Lecun, Bottou, Bengio, and Haffner}}}
\bibcite{DBLP:journals/corr/YuK15}{{6}{2015}{{Yu \& Koltun}}{{Yu and Koltun}}}
